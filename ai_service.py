# -*- coding: utf-8 -*-
"""(Part-6)AI-Service(Template) - Updated Aug-2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12pm135aA572stOUctJwGOLYMf8LIv6L2
"""

# 필요 Library install
"""
이 셀을 실행 한 후 다음의 에러 팝업이 뜰 수 있음.

WARNING: The following packages were previously imported in this runtime:
  [_distutils_hack]
You must restart the runtime in order to use newly installed versions.

Restarting will lose all runtime state, including local variables.
"""
!pip install transformers==4.55.2 peft==0.17.0 trl==0.21.0 bitsandbytes==0.47.0 accelerate==1.10.0 vllm==0.10.1 gradio==5.43.0 pydantic==2.11.7
!pip install ipython>=8.0 jedi>=0.19

# HF token 설정
from huggingface_hub import notebook_login
notebook_login()

# Google Drive Import
from google.colab import drive
drive.mount('/content/drive')

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
from vllm import LLM, SamplingParams
import gradio as gr

vllm_model = LLM(
    model = "psh3333/llama3-alpaca-tuned-and-merged",
    tokenizer = "psh3333/llama3-alpaca-tuned-and-merged",
    gpu_memory_utilization=0.85,
    enable_lora = True
)

sampling_params = SamplingParams(temperature=0.05, top_p=0.95, max_tokens=256)

from vllm.lora.request import LoRARequest

dpo_output_1_base_path = "/content/drive/MyDrive/dpo_output_1/"
dpo_output_2_base_path = "/content/drive/MyDrive/dpo_output_2/"

lora_configs = {
    "dpo_output_1_cpk_10": (1, dpo_output_1_base_path + "checkpoint-10"),
    "dpo_output_1_cpk_20": (2, dpo_output_1_base_path + "checkpoint-20"),
    "dpo_output_1_cpk_30": (3, dpo_output_1_base_path + "checkpoint-30"),
    "dpo_output_1_cpk_40": (4, dpo_output_1_base_path + "checkpoint-40"),
    "dpo_output_2_cpk_10": (5, dpo_output_2_base_path + "checkpoint-10"),
    "dpo_output_2_cpk_20": (6, dpo_output_2_base_path + "checkpoint-20"),
    "dpo_output_2_cpk_30": (7, dpo_output_2_base_path + "checkpoint-30"),
    "dpo_output_2_cpk_40": (8, dpo_output_2_base_path + "checkpoint-40")
}

def generate_text(raw_input, temperature = 0.05, top_p = 0.95, max_tokens=256, lora_mode = "default"):
    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens
    )
    alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

    ### Instruction:
    {}

    ### Response:
    {}"""
    prompt = alpaca_prompt.format(raw_input,"")
    lora_config = {}



    if lora_mode != "default":
        lora_config["lora_request"] = LoRARequest(lora_mode, lora_configs[lora_mode][0], lora_configs[lora_mode][1])

    outputs = vllm_model.generate(
        [prompt],
        sampling_params,
        **lora_config,
    )
    return outputs[0].outputs[0].text

iface = gr.Interface(
    fn=generate_text,
    inputs=[
        gr.Textbox(label="Prompt"),
        gr.Slider(0.0, 2.0, value=0.0, label="Temperature"),
        gr.Slider(0.0, 1.0, value=0.95, label="Top P"),
        gr.Slider(1, 1000, value=256, step=1, label="Max Tokens"),
        gr.Dropdown(
            choices=["default"] + list(lora_configs.keys()),
            label="Model Type",
            value="default"
        ),
    ],
    outputs="text",
    title="Fine-tuned PEFT Model Demo",
    description="Enter a prompt to generate text using the fine-tuned model.",
    api_name = "generate"
)

iface.launch(share=True)

iface.close()

