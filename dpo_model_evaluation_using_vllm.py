# -*- coding: utf-8 -*-
"""(Part-4)DPO-model-evaluation-using-vLLM(template)  - Updated Aug-2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ChHh2NMoHsT96EBX38OgLjuvCGXr3mUA
"""

# 필요 Library install
"""
이 셀을 실행 한 후 다음의 에러 팝업이 뜰 수 있습니다.
정상적인 것이니 Restart session 버튼을 누르고 다음 셀로 진행하시면 됩니다.

WARNING: The following packages were previously imported in this runtime:
  [_distutils_hack]
You must restart the runtime in order to use newly installed versions.

Restarting will lose all runtime state, including local variables.
"""
!pip install transformers==4.55.2 peft==0.17.0 trl==0.21.0 bitsandbytes==0.47.0 accelerate==1.10.0 vllm==0.10.1 gradio==5.43.0 pydantic==2.11.7
!pip install ipython>=8.0 jedi>=0.19

# HF token 설정
from huggingface_hub import notebook_login
notebook_login()

# Google Drive Import
from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/fine_tune_output

!ls /content/drive/MyDrive/dpo_output_1

!ls /content/drive/MyDrive/dpo_output_2

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
from vllm import LLM, SamplingParams
import gradio as gr

from vllm import LLM, SamplingParams

vllm_model = LLM(
    model = "psh3333/llama3-alpaca-tuned-and-merged",
    tokenizer = "psh3333/llama3-alpaca-tuned-and-merged",
    gpu_memory_utilization=0.85,
    enable_lora = True
)

sampling_params = SamplingParams(temperature=0.05, top_p=0.95, max_tokens=256)

from vllm.lora.request import LoRARequest

dpo_output_1_base_path = "/content/drive/MyDrive/dpo_output_1/"
dpo_output_2_base_path = "/content/drive/MyDrive/dpo_output_2/"

lora_configs = {
    "dpo_output_1_cpk_10": (1, dpo_output_1_base_path + "checkpoint-10"),
    "dpo_output_1_cpk_20": (2, dpo_output_1_base_path + "checkpoint-20"),
    "dpo_output_1_cpk_30": (3, dpo_output_1_base_path + "checkpoint-30"),
    "dpo_output_1_cpk_40": (4, dpo_output_1_base_path + "checkpoint-40"),
    "dpo_output_2_cpk_10": (5, dpo_output_2_base_path + "checkpoint-10"),
    "dpo_output_2_cpk_20": (6, dpo_output_2_base_path + "checkpoint-20"),
    "dpo_output_2_cpk_30": (7, dpo_output_2_base_path + "checkpoint-30"),
    "dpo_output_2_cpk_40": (8, dpo_output_2_base_path + "checkpoint-40")
}

def generate_text(raw_input, temperature = 0.05, top_p = 0.95, max_tokens=256, lora_mode = "default"):
    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens
    )
    alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

    ### Instruction:
    {}

    ### Response:
    {}"""
    prompt = alpaca_prompt.format(raw_input,"")
    lora_config = {}



    if lora_mode != "default":
        lora_config["lora_request"] = LoRARequest(lora_mode, lora_configs[lora_mode][0], lora_configs[lora_mode][1])

    outputs = vllm_model.generate(
        [prompt],
        sampling_params,
        **lora_config,
    )
    return outputs[0].outputs[0].text

questions = [
    "List three ways to reduce plastic waste in daily life.",
    "Write a haiku about artificial intelligence",
    "How can I improve my public speaking skills?",
    "AI 분야에서 사용하는 LLM이라는 용어가 뭔지 설명해줘",
    "What is a famous tall tower in Paris?",
    "What is Fine-Tuning?",
    "15와 25의 최소공배수를 구하시오.",
    "제2차 세계대전의 주요 원인은 무엇이었나요?",
    "광합성 과정을 간단히 설명해주세요.",
    "셰익스피어의 '햄릿'에서 주인공의 성격을 분석해보세요.",
    "기후 변화가 전 세계적으로 미치는 영향에 대해 설명해주세요.",
    "파이썬에서 리스트와 튜플의 차이점은 무엇인가요?",
    "인공지능은 의식을 가질 수 있을까요? 그 이유는 무엇인가요?",
    "'안녕하세요, 오늘 날씨가 좋네요'를 영어로 번역해주세요.",
    "우주 여행을 주제로 한 짧은 이야기를 만들어보세요.",
    "인플레이션이 경제에 미치는 영향을 설명해주세요."
]

for idx, question in enumerate(questions):
  print(f"Question_{idx}: {question}")
  print("Default model result -----")
  print(generate_text(raw_input = question, lora_mode = "default"))
  for mode in lora_configs.keys():
    print(f"{mode} model result ------")
    print(generate_text(raw_input = question, lora_mode = mode))
    print()