# -*- coding: utf-8 -*-
"""(Part-4)merge_and_unload(template)  - Updated Aug-2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dZm9oKQMjEfecuRvsXEMCzhI2mrg1o93
"""

# 필요 Library install
!pip install transformers==4.48.0 peft==0.14.0 trl==0.13.0 bitsandbytes==0.45.3 accelerate==1.2.1

# HF token 설정
from huggingface_hub import notebook_login
notebook_login()

# Transformer Model 로드
# quantization 없이 로드
from transformers import AutoModelForCausalLM
import torch
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B",
    torch_dtype=torch.bfloat16,
    device_map = {"": 0}
)

# Google drive mount
from google.colab import drive
drive.mount('/content/drive')

fine_tuned_adapter_path = '/content/drive/MyDrive/fine_tune_output/checkpoint-50'

# 16bit model + adapter
from peft import PeftConfig, PeftModel
base_and_adapter_model = PeftModel.from_pretrained(model, fine_tuned_adapter_path)

base_and_adapter_model = base_and_adapter_model.merge_and_unload()

# Tokenizer 설정
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(fine_tuned_adapter_path)

model_path = "psh3333/llama3-alpaca-tuned-and-merged"

base_and_adapter_model.push_to_hub(model_path)

tokenizer.push_to_hub(model_path)